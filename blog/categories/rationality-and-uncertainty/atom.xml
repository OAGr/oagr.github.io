<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Rationality and Uncertainty | Ozzie Gooen]]></title>
  <link href="http://OAGr.github.io/blog/categories/rationality-and-uncertainty/atom.xml" rel="self"/>
  <link href="http://OAGr.github.io/"/>
  <updated>2013-09-16T23:34:29+01:00</updated>
  <id>http://OAGr.github.io/</id>
  <author>
    <name><![CDATA[Ozzie Gooen]]></name>
    <email><![CDATA[ozzie.gooen@80000hours.org]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Morality is Still on Very Flaky Foundations]]></title>
    <link href="http://OAGr.github.io/blog/2013/05/10/morality-on-flaky-foundations/"/>
    <updated>2013-05-10T00:00:00+01:00</updated>
    <id>http://OAGr.github.io/blog/2013/05/10/morality-on-flaky-foundations</id>
    <content type="html"><![CDATA[<blockquote>
  <p>This was a post I wrote several months ago in response to what I knew about effective altruism movement in the Bay area.  I now have met other EA communites and am more uncertain.  However, I believe it still raises some interesting points and could be useful to others.  </p>
</blockquote>

<h2 id="the-original-problem">The Original Problem</h2>

<p>Most people I know consider themselves to be relatively moral individuals, no matter what kind of morality they ascribe to.  Almost every single piece of literature we read and a large majority of politics hangs onto the ideal of moral righteousness in one form or another.  So when people like <a href="https://en.wikipedia.org/wiki/Peter_Singer">Peter Singer</a> and <a href="https://en.wikipedia.org/wiki/Eliezer_Yudkowsky">Eliezer Yudiskowsky</a> goes out and state that <a href="http://www.utilitarian.net/singer/by/199704--.htm">conventional morality is bullshit</a>, people <a href="https://en.wikipedia.org/wiki/Peter_Singer#Criticism_of_Singer">freak out</a>. The basic argument goes that we typically judge morality based on initions we have, rather than facts and rigorous analysis.  And this leads to <a href="http://intelligence.org/files/CognitiveBiases.pdf">strange things</a>, like seeking moral vengeance against people who kill one or two individuals, while not blaming people with the ability to save millions of lives but not doing so.  Our intuitions were created from evolution optimized for cavemen, and whatever social norms and media we are subjected to.  They are are faulty due to numerous <a href="http://intelligence.org/files/CognitiveBiases.pdf">cognitive biases</a>, and a moral standing upon them is difficult to de-couple from complete  <a href="https://en.wikipedia.org/wiki/Moral_relativism">moral relativism</a>, an incredibly undesirable state for intellectual morality. So in a way, most modern morality is built upon strong intuitions.  The issue is that these intuitions are inconsistent and provably flawed for the modern era.  If we live out our entire lives and build societies based on these moral intuitions, then we are arguably doing almost everything on top of some very shaky foundations. I’ll use a few comics from the excellent <a href="http://www.amazon.com/Logicomix-An-Epic-Search-Truth/dp/1596914521/ref=sr_1_1?ie=UTF8&amp;qid=1368222515&amp;sr=8-1&amp;keywords=logicomix">Logicomix</a> to help illustrate my point.  That book detailed the mathematical venture into complete proof and certainty against intuition, and I feel like that was very similar to what we are now going through with morality.  </p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/basic.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/basic.png" alt="basic" /> </a> </p>

<h2 id="rationality-to-the-rescue">Rationality to the Rescue</h2>
<p>How to we fix this?  The existing fight against moral intuition has come from  utilitarianism and effective altruism.  These groups try to deduce why our important intuitions exist to help prove which ones are morally true and which are cultural and/or evolutionary artifacts.  Utilitarianism was somewhat of an attempt to create an overal structure that is somewhat intuitively reasonable but most importantly, numerical.  Basically, we define “goodness” in some way (often defined as “happiness”, but possibly “awesomeness” or any other word you might consider to be a reasonable ultimate human goal) and define moral good as to whatever reaches that.  The Effective Altruist group kind of grew out of this to help define good life practices (like donating to efficient charities rather than cute ones) that help optimize this number.  Often this seems unintuitive (for example, it may be very justifiable to not give to homeless people close to you and instead donate to charities you will never see), but can be defended well with math. Basic practices here are to do things like encouraging people to live lives that favored large donations to efficient charities, rather than typical bleeding-heart goodwill.  Others work on projects “orders of magnitude” more useful than anyone else in mainstream society, including researching the Singularity, ending human aging, and human computer uploading.  On paper these are incredible useful activities, because they are so much better than what is mainstream.  People doing these can feel quite morally superior to the rest of society, and still feel the logically reasonable ability to complain morally about war criminals and resist moral relativism.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/line.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/line.png?w=922" alt="line" /> </a></p>

<p>Above shows a basic diagram of how I see the the “morality” of individuals based on something like utilitarianism.  While I chose “Expected Average Lives Saved (or Equivalent)”, what we are really talking about is some measure of goodness.  Peter Singer well reasons that people who choose not to do good should be faced with similar scrutiny as those who choose to do harm.  Likewise, the only “moral” position is to do as “much as possible”, which is generally defined as “Effective Altruism”, that box on the right.  The groups mentioned all fit into this general category, and from their perspective, they are all somewhat in the same position.</p>

<h2 id="trouble-in-paradise">Trouble In Paradise</h2>

<p>This all works out well, until even more effective rationalists appear.  Perhaps they come in the form of strict <a href="https://en.wikipedia.org/wiki/Machine_Intelligence_Research_Institute">AI-risk reducers</a>, or something else we haven’t seen yet.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/line2.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/line2.png" alt="line2" /> </a></p>

<p>Keep in mind that each bar is an order of magnitude greater than the last one, so in comparison to the “more effective altruists”, the “effective altruists” look 10x to 100x as suboptimal.  The “effective altruists” claim that regular people were imoral because they only save 1 life rather than 1000 (resulting in 999 unnecessary deaths), but here come this other group complaining that the “effective altruists” are saving 1k rather than 100k lives (resulting in 99,000 unnecessary deaths).</p>

<p>The trouble is that one of the main justifications for the work of many effective altruists is that they are much “better in comparison” to everyone else.  Being “better” only makes one “good” when one only cares about comparisons.  Was the “best” Nazi a “good” person, because he or she should be compared to other Nazis, or a “bad” one, because we happen to have non-nazis to compare him or her to?  If one’s morality is rooted in its comparison to that of other’s, then one can go from “good” to “bad” just by the appearance of a new group.</p>

<h2 id="the-annoying-pupils">The Annoying Pupils</h2>

<p>Rejecting intuitive solutions presents all kinds of other nasty issues, if you were to take it to the extreme.  This would mean that there is almost no point where one can really say “oh, this makes basic sense, so I’ll accept this premise and work from there”.  And when one decides to create an entire personal philosophy off of preferring rationality to intuition, the rabbit hole can go much deeper than individual prefers.  At every level of logical thought, there can be someone new who complains that it’s not completely rational, and little rational reason for proving that it shouldn’t be.</p>

<p>Typically when we make logical decisions, we ultimately come down to a few intuitive things.  For example, we may optimize a business strategy for revenue, but we still make the intuitive decision that revenue is what we should be optimizing for.  One can go further and “rationally” explain why revenue is statistically expected to be the best way for the business to grow, but from there, there’s the intuitive assumption that the growth of the business is a good thing.  Several other intuitive assumptions are probably made in the rest of the model as well, it is almost impossible to make a complete model without them.</p>

<p>So the big question is how to decide where to make intuitive judgements vs. rational ones.  But to decide this question, there should ideally be a rational answer.  If there were an intuitive one, that would mean an intuitive foundation, which leads us back to where we started.  And finding a rational answer that we agree upon can be incredibly difficult.</p>

<h2 id="levels-of-goodness-intuition">Levels of Goodness Intuition</h2>

<p>Let’s begin with an example.  When deciding on donating to a poorly run childcare charity compared to a well run one, the choice is relatively obvious within most definitions of “good”.  So within many definitions of “good”, the answer could be made relatively easily.</p>

<p>But take something more extreme: what if we need to choose between the lives of 4 cows compared to 1 human being?  Or 1000 <a href="http://www.utilitarian-essays.com/humane-insecticides.html">insects</a> (expected, though highly unlikely) compared to 1 human being?</p>

<p>The typical answer that I’ve heard for this, is to make an intuitive judgement about how important a cow is to a human.  Perhaps cows are 1/3rd as intelligent, then 4 cows “utility” would be greater than that of one human.  But the “utility” is a very difficult thing to define, and many people really only care to optimize human utility.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/human.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/human.png" alt="human" /> </a></p>

<p>Even worse is when this taken to the computational extreme.  Many futurists advocate human brain <a href="http://wiki.lesswrong.com/wiki/Mind_uploading">uploading to machines</a>.  They claim that simulated cognition is equivalent to physical cognition, and simulations of us could be much better taken care of than real ones.  Others find this scenario as repulsive and wrong.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/physical.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/physical.png" alt="physical" /> </a></p>

<p>Some take another step and say that instead of us being uploaded into an amazing world, we should go straight to <a href="http://wiki.lesswrong.com/wiki/Wireheading">wireheading</a>, into a state where our brains get pure “happiness” without realized experiences.  This is very controversial but seems very counter-intuitive and strange to other values we have.  As <a href="http://wiki.lesswrong.com/wiki/Wireheading">lesswrong</a> points out,</p>

<blockquote>
  <p>“A civilization of wireheads “blissing out” all day while being fed and maintained by robots would be a state of maximum happiness, but such a civilization would have no art, love, scientific discovery, or any of the other things humans find valuable.”</p>
</blockquote>

<p>Some take this further and say that if simulations of us are more “optimal” than us, then wouldn’t simulations of even “happier” beings be more optimal?  Or perhaps an A.I. should just simulate “pleasure” itself, essentially ending living sentience as we know it and replacing it completely with machines simulating things we can not now possibly imagine.  There are some that feel like for any given definition of “happiness”, this would be preferable, but others who feel like it is morally wrong for us because of its lack of most existing human values.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/terminator.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/terminator.png" alt="terminator" /> </a></p>

<p>The one “completely rational” utilitarian answer that makes sense (is intuitive as being rational) to me is just that, that ultimately the most utility would come from replacing all current sentience with completely artificial beings (or being).  Essentially, not only letting the Matrix or Terminator overlords win the fight, but rushing for this to happen, fighting for it, and doing whatever is necessary.  On the strictest definitions of utility I could imagine, this would be the most analytically optimal thing.  Of course, that is for now, until I hear of another idea that could be 10x to 100x as “optimal” as this one.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/line3.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/line3.png" alt="line3" /> </a></p>

<p>But of course, essentially all people reject this final bit.  The wire heading promoters will say that they are promoting the best outcome (and thus are doing by far the most efficient utilitarian work), and claim that the “terminator” scenario is morally apprehensible because of how wrong it _feels _in comparison.   The uploading enthusiasts will promote theirs as the best outcome while claiming that the wire heading outcome is unnatural and obviously wrong.  The human utopians will promote theirs as the best outcome while claiming that human uploading is un-human in comparison.  And local charity enthusiasts will insist that their work is the best, and the intuitive good feelings it provides would be missed by the “effective altruists”, and thus would wouldn’t be reasonable.</p>

<p>Also, I’ll note that quite a few people in the field of futuristic high-technology “effective altruism” are very much intent on themselves being a part of this.  That they themselves would get to live forever or be uploaded.  When asked why they prefer this to something that could mathematically produce much higher expected value, I hear answers like “this is already good enough!”  But how do we possibly decide rationally what “good enough” means, especially when there is something obvious better?</p>

<p>The great difficulty here, as I see it, is that the extreme moral stance, with as little intuition as possible, seems to be the terminator scenario or similar, areas that seems completely morally strange and unintuitive to absolutely everybody.  But if one were to do more “reasonable” things, one may make order-of-magnitude losses in efficiency for the sake of achieving a more intuitive answer, which was the entire problem we started out against.  We could make a trade-off in between (like many I know do), but such a trade-off would also be an intuitive one.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/05/second.png"> <img src="http://bowlabs.files.wordpress.com/2013/05/second.png" alt="second" /> </a></p>

<h2 id="where-to-go-from-here">Where to Go From Here</h2>

<p>If one does accept some intuitive limitations from terminator support (or other intuitive options like keeping 10% of income because it “feels right”), at least we could admit it and realize that we aren’t that much different from those “inefficient altruists” or even “regular people” who make different intuitive limitations than we do.  This is in many ways moral relativism and leads to a vague and somewhat frustrated form of morality, but at least not a hypocritical one.  Being honest to ourselves is a good first step.</p>

<p>I believe that really finding a rational ends that we can agree upon will take quite a bit of change to our own intuitions   It may just be a matter of time, like it was to get reduce other limitions (nationalism, sexism, racism, etc).  Of course, the issue is that the time required may be longer than available before apocalyptic / world changing event, so it is quite likely this will never be settled.</p>

<p>I do believe that this should be figured out and discussed much more rigorously.  Because from what I’ve seen, many of <strong><em>the most important</em></strong> moral issues are still surrounded by relatively emotional and intutive debate.  And for us being “rationality nerds”, this is quite a huge deal.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to Tell if a Web Company is Evil]]></title>
    <link href="http://OAGr.github.io/blog/2013/04/30/how-to-tell-if-a-web-company-is-evil/"/>
    <updated>2013-04-30T00:00:00+01:00</updated>
    <id>http://OAGr.github.io/blog/2013/04/30/how-to-tell-if-a-web-company-is-evil</id>
    <content type="html"><![CDATA[<p>I’m sick of companies cherry picking specific features to explain how they help people.  Specific features will always exist, and the existence of them ruins the point.  The big question is, is it really the purpose of the company to help people, or is helping people an occasional externality of other goals they are trying to achieve?  It’s hard to give someone credit for a positive action that was done for a malicious or unrelated purpose.<br />
<a href="http://bowlabs.files.wordpress.com/2013/04/basic.png"> <img src="http://bowlabs.files.wordpress.com/2013/04/basic.png?w=590" alt="Altruistic vs. Power Ven Diagram" title="Altruistic vs. Power Ven Diagram" /> </a></p>

<p>Here’s an Altruism/Power Ven Diagram.  On the left are decisions a company makes to help people, on the right are decisions that give them power.  Figuring out the intentions of a company may have little to do with how much they are helping people, because that intersection may be quite large.  The bigger question is how they pay attention to the other parts of the diagram.</p>

<p><a href="http://bowlabs.files.wordpress.com/2013/04/people.png"> <img src="http://bowlabs.files.wordpress.com/2013/04/people.png?w=300" alt="Optimizing for People" /> </a>  </p>

<p>If a company were optimizing for people, it’s actions may look like this.  </p>

<p><a href="http://bowlabs.files.wordpress.com/2013/04/shareholders.png"> <img src="http://bowlabs.files.wordpress.com/2013/04/shareholders.png?w=300" alt="If a company optimized for its corporate shareholders, it's actions would be here." /> </a>  </p>

<p>If a company optimized for its corporate shareholders, it’s actions would be here.  </p>

<p>The big question is which of these two above graphs better represents the actions of a company, not how much there is of an intersection to show to the press.  Here’s a graph of what I see as being decisions in each sector, you can decide for yourself what the aims of your favorite web companies may be.  </p>

<p><a href="http://bowlabs.files.wordpress.com/2013/04/web-applications.png"> <img src="http://bowlabs.files.wordpress.com/2013/04/web-applications.png?w=300" alt="Web Application Altruism/Power Daigram" /> </a><br />
Web Application Altruism/Power Diagram</p>

<h2 id="doesnt-power-eventually-help-users">Doesn’t Power Eventually Help Users?</h2>
<p>This is the main argument from all power grabbers, ever.  That “with more resources, we can do more good,” so decisions to increase power eventually do help users.  While this can be used to justify almost any action, there does exist the fact that without power even the most idealistic or altruistic groups will not succeed.  It also makes sense that those in power will be those optimized to get power over all other things. It may be that most existing companies (organizations, really) only exist to gain power, where the argument is that the power will eventually be (and occasionally is) used for the goals of these organizations (which are supposedly things other than pure shareholder value). But here, how do you identify groups with good intentions from the rest?  From the outside view, they look almost exactly the same, because they are optimizing for power whenever options are presented. It is true that trade-offs need to be made, but for them to be made well there should be some greater model and rigorous attention to the tradeoffs.  Could companies sponsor studies on how to optimize the numeric trade-offs between short term power grabs and long term civilian value?  It may be hard, but given that these organizations sometimes seem to actually care (or want to care) about value to society, it probably wouldn’t cost much compared to other things they are doing. </p>

<h2 id="the-economic-near-ideal">The Economic Near-Ideal</h2>
<p>Of course, one solution would be to create companies where there are few trade-offs.  One can argue that companies like <a href="https://www.10gen.com/">10gen</a> have nice models where they very well align their own power increases with societal benefit.  </p>

<p><a href="http://bowlabs.files.wordpress.com/2013/04/ideal.png"> <img src="http://bowlabs.files.wordpress.com/2013/04/ideal.png?w=300" alt="The economic near-ideal" /> </a>  </p>

<p>Here companies aren’t defined by how good they are trying to be, but by how much their selfish actions are correlated with altruistic ones.  Here the decision is much more about deciding what kind of company to make (and for outsiders, deciding rules and publicity), then deciding what features to implement.  Yet it also may mean that the more altruistic people will favor industries where this correlation is high, leaving other industries as easy prey for less idealistic people.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Awards imply false certainty]]></title>
    <link href="http://OAGr.github.io/blog/2013/02/28/awards-imply-false-certainty/"/>
    <updated>2013-02-28T00:00:00+00:00</updated>
    <id>http://OAGr.github.io/blog/2013/02/28/awards-imply-false-certainty</id>
    <content type="html"><![CDATA[<p>In high school I got sick of most award systems.  Prizes were given like “best science student” or “2nd best musician” like “best” really meant something.  I also was questionable of whether the negative externalities created by these systems offset the positive benefit for the recipients and close friends, but I think the more pressing issue is the implied certainty. </p>

<p><a href="http://bowlabs.files.wordpress.com/2013/02/donc355.jpg"> <img src="http://bowlabs.files.wordpress.com/2013/02/donc355.jpg" alt="obligatory average photograph of standard trophies" /> </a><br />
Obligatory average photograph of standard trophies</p>

<p>What the hell does it mean to be the “best science student”?  Most of the time people have little idea what “best” actually refers to, but rather assume it’s within a space of shared understanding that the local community will agree upon.  This is not always true, especially when there are disagreements about the winner.  Should the “best” book be one with <a href="http://www.newyorker.com/online/blogs/books/2012/07/letter-from-the-pulitzer-fiction-jury-what-really-happened-this-year.html">more character emphasis, or verbal eloquence</a>?  In some cases it’s obvious how to rank items according to this vague space of shared understanding, but in other cases it’s apparent that the space isn’t narrow at all. </p>

<p>Even this assumes a level of consistency of mindset and competency of judges, which may be and often is highly questionable.  Just because a person may be an “expert” at their field doesn’t mean they are any good at it.  They may just be the best of ignoramuses. </p>

<p>We can of course overcome this in part by assigning high uncertainty to our prizes and phases. </p>

<blockquote>
  <p>“This award is given to the person who our team of three specific, somewhat randomly chosen but assumingly competent (according to our school’s current department standards)  people.  Using our current understanding of what likely makes up “excellent”, we have predicted that this entree is the most likely to be the best one.  If we had chosen a different group of judges from a similar group and this had been done again, we predict that there would be a 40% chance of this entree still winning 1st place.” </p>
</blockquote>

<p>Of course, few organizations will ever do this.  It hurts the often false sense of their own authority.  It makes the competition seem rather unimportant, even though it likely is. </p>

<h2 id="my-counterplan">My CounterPlan</h2>

<p>In high school I designed a small representation of this, in a “Seal of Perceived Excellence”.  I finally purchased a custom stamp in the beginning of college.  I considered it accurate on my homework, when I would spend a lot of time doing a job I thought was decent, but knowing I probably got some fundamental elements wrong and had little idea of how well I actually did.  While now I look back on it and find the design a bit childish, I still like the point and would later be interested in extending it elsewhere.  <a href="http://bowlabs.files.wordpress.com/2013/02/seal.jpeg"> <img src="http://bowlabs.files.wordpress.com/2013/02/seal.jpeg" alt="Ozzie Gooen Seal of Perceived Excellence" /> </a> Ozzie Gooen Seal of Perceived Excellence</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Why obsessives may hate scheduling meetings]]></title>
    <link href="http://OAGr.github.io/blog/2013/02/27/why-obsessives-hate-meetings/"/>
    <updated>2013-02-27T00:00:00+00:00</updated>
    <id>http://OAGr.github.io/blog/2013/02/27/why-obsessives-hate-meetings</id>
    <content type="html"><![CDATA[<p><a href="http://bowlabs.files.wordpress.com/2013/02/b1_north_hallway1.jpg"> <img src="http://bowlabs.files.wordpress.com/2013/02/b1_north_hallway1.jpg" alt="Not Harvey Mudd, but close." /> </a></p>

<p>I remember at <a href="http://www.hmc.edu/">Harvey Mudd College</a> I would often be really interested in really specific technical topics and go through great difficulties to talk to anyone about them, especially the professors.  Many of the engineering professors would be out during all of the convenient times for me (after 5pm M-T, and most of Friday because most seemed to be out consulting).   Sure I could schedule meetings, but that didn’t feel right.  I wanted to walk in and start talking. </p>

<p>It recently dawned on me what very quick discussion was so important to me.  It was because I knew I would loose interest quickly and wanted to get my thoughts out while they were with me.  I would often hours or days on end obsessing on an issue, and all of it was fresh and new to my memory right then.  Scheduling a meeting a week after would throw everything off. Not only was it not convenient, but I had no way of knowing what I would be interested in the next week. I notice this now.  I plan meetings about partnerships, then get really excited about product development and loose interest in the meetings.  It makes it really hard to hold discussions with other people in the area.  <strong>Therefore it seems very important to have a large variety of people in my general vicinity that will handle my somewhat unpredictable interests while being close enough not to have to schedule an appointment any more than a few hours away.</strong> </p>

<p>Fortunately I’m living with 17 other smart people in <a href="http://risesf.com/">Rise SF</a>, which definitely is a plus.  But the location is restricting (Twin Peaks, SF) and I don’t interact routinely with large groups of people outside the house. Maybe this is one reason why <a href="http://en.wikipedia.org/wiki/Building_20">building 20</a> and similar tightly packed intelectual networks work so well.  Even if people <strong>can</strong> meet each other doesn’t mean it will be convenient; them being able to stop in at a whim may be significantly more efficient and productive.  This way they can discuss what they are interested in, in the moment they are interested in it.  </p>

<p>More information on <a href="http://www.newyorker.com/reporting/2012/01/30/120130fa_fact_lehrer?currentPage=all">Building 20</a> here, toward the end.  It’s a fascinating read.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Most of what I'm writing is probably wrong]]></title>
    <link href="http://OAGr.github.io/blog/2013/02/25/most-of-what-im-writing-is-probably-wrong/"/>
    <updated>2013-02-25T00:00:00+00:00</updated>
    <id>http://OAGr.github.io/blog/2013/02/25/most-of-what-im-writing-is-probably-wrong</id>
    <content type="html"><![CDATA[<p>The world is really complicated.  So when I, or anyone else comes across with a point, we’re doing so with a very limited set of information.  Even the <a href="http://techcrunch.com/2012/11/07/pundit-forecasts-all-wrong-silver-perfectly-right-is-punditry-dead/">experts</a> typically get things wrong.  That makes it pretty hopeless for anyone else.</p>

<p>That said, we need to pretend we know things.  Why?  Because everyone else does, and if you admit that you don’t know things people will believe you.  But they will continue believing those who pretend to know things, so you’ll look worse in comparison.  It’s like a bizarre athletic contest where everyone is cheating with hacks and steroids and those who don’t quickly get kicked out for not doing well in comparison.  </p>

<p>I’m quite sure that one can only read the intelligence of less intelligent people.  It’s kind of like Jazz; to an untrained ear <a href="http://en.wikipedia.org/wiki/Charlie_Parker">Charlie Parker</a> could sound as good as some amateur enthusiast.  I realized this quickly when I played jazz; no matter how good I actually was, if I played loud and confident during my solos people would assume I knew more than them and was actually quite impressive if only they understood jazz a bit better.  </p>

<p>So when I talk I’ll continue to state things like they are true, even though I completely realize that most of them are highly uncertain.  I’ve been considering making a new language for this similar to <a href="http://en.wikipedia.org/wiki/E-Prime">E-prime,</a> but it probably wouldn’t help much for normal conversation.</p>

<p>This is important to accept because otherwise I would never say things.  I realize that much of what I talk about has been discussed before.  I could theoretically research every subject to its academic end and argue the points for those who know where the subject elite stand.  But the only people who typically do this, are the PHDs who spend a few years doing nothing but researching the literature to find this end.  Which means I would be constrained to one very specific field, and spend several years of life achieving that.  </p>

<p>What I’m saying is that the ends don’t matter too much because they are likely to be wrong.  And there’s probably some very interesting optimization process in deciding how much information you have to know about an argument to be able to contribute to it.  But the most important information is buried within the individual arguments and ideas to back them up.  The outcomes and weighing of those arguments is less important because there are probably many that I’m unaware of given my limited expertise.  </p>

<p>So I’ll continue to talk and listen to things that interest me, no matter how much I know about them.  And even if my general mentality is wrong (it likely will be much of the time), I hope that I have things to say within them that may be useful to the greater information.</p>

<p> </p>
]]></content>
  </entry>
  
</feed>
